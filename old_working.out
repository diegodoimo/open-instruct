[2024-04-16 22:03:10,192] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/orfeo/cephfs/scratch/area/ddoimo/open/open-instruct/env_amd/lib/python3.11/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
[2024-04-16 22:05:23,111] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/orfeo/cephfs/scratch/area/ddoimo/open/open-instruct/env_amd/lib/python3.11/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/orfeo/cephfs/scratch/area/ddoimo/open/open-instruct/env_amd/lib/python3.11/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
04/16/2024 22:05:34 - INFO - __main__ - Distributed environment: DistributedType.NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: bf16

loading configuration file /orfeo/cephfs/scratch/area/ddoimo/llama/llama_v1/models_hf/7B/config.json
Model config LlamaConfig {
  "_name_or_path": "/orfeo/cephfs/scratch/area/ddoimo/llama/llama_v1/models_hf/7B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.35.0.dev0",
  "use_cache": true,
  "vocab_size": 32000
}

loading file tokenizer.model
loading file added_tokens.json
loading file special_tokens_map.json
loading file tokenizer_config.json
loading file tokenizer.json
Adding <s> to the vocabulary
Adding </s> to the vocabulary
Adding <unk> to the vocabulary
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
tokenizer loaded. 


model_loading started. 


LlamaConfig {
  "_name_or_path": "/orfeo/cephfs/scratch/area/ddoimo/llama/llama_v1/models_hf/7B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.35.0.dev0",
  "use_cache": true,
  "vocab_size": 32000
}

loading weights file /orfeo/cephfs/scratch/area/ddoimo/llama/llama_v1/models_hf/7B/pytorch_model.bin.index.json
Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.
Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2
}

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:39<00:39, 39.80s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:52<00:00, 24.15s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:53<00:00, 26.55s/it]
All model checkpoint weights were used when initializing LlamaForCausalLM.

All the weights of LlamaForCausalLM were initialized from the model checkpoint at /orfeo/cephfs/scratch/area/ddoimo/llama/llama_v1/models_hf/7B.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
loading configuration file /orfeo/cephfs/scratch/area/ddoimo/llama/llama_v1/models_hf/7B/generation_config.json
Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2
}

model loading finished. 


model loaded. 


Assigning <s> to the bos_token key of the tokenizer
Assigning </s> to the eos_token key of the tokenizer
Assigning <unk> to the unk_token key of the tokenizer
Assigning <pad> to the pad_token key of the tokenizer
Adding <s> to the vocabulary
Adding </s> to the vocabulary
Adding <unk> to the vocabulary
Adding <pad> to the vocabulary
You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 32001. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc
model embedding resized. 


04/16/2024 22:07:46 - INFO - __main__ - Initializing LORA model...
trainable params: 159,907,840 || all params: 6,898,331,648 || trainable%: 2.3180654129083704
start preprocessing the data. 


/orfeo/cephfs/scratch/area/ddoimo/open/open-instruct/env_amd/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by promote_options='default'.
  table = cls._concat_blocks(blocks, axis=0)
finished preprocessing. 


14847
loading dataframes
/orfeo/cephfs/scratch/area/ddoimo/open/open-instruct/env_amd/lib/python3.11/site-packages/datasets/table.py:1395: FutureWarning: promote has been superseded by promote_options='default'.
  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]
/orfeo/cephfs/scratch/area/ddoimo/open/open-instruct/env_amd/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by promote_options='default'.
  table = cls._concat_blocks(blocks, axis=0)
loading dataframes
/orfeo/cephfs/scratch/area/ddoimo/open/open-instruct/env_amd/lib/python3.11/site-packages/datasets/table.py:1395: FutureWarning: promote has been superseded by promote_options='default'.
  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]
/orfeo/cephfs/scratch/area/ddoimo/open/open-instruct/env_amd/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by promote_options='default'.
  table = cls._concat_blocks(blocks, axis=0)
you filter out 5 examples,  0.04% of the total
start training
CUDA mem allocated: 12.88104248046875 GB
CUDA mem reserved: 12.896484375 GB
before train run
evaluating mmlu
996/ 1531 inputs processed
baseline average mmlu test accuracy: 0.3220
CUDA mem allocated: 21.04541301727295 GB
CUDA mem reserved: 26.4765625 GB
before after evaluate
tot iter: 14847
  Step: 20, LR: 9.380530973451328e-05, Loss: 1.4018185615539551, Time:  0.12 hours
  Step: 40, LR: 8.495575221238938e-05, Loss: 1.2566618919372559, Time:  0.23 hours
evaluating mmlu
996/ 1531 inputs processed
baseline average mmlu val accuracy: 0.3357, time  0.60 min
  Step: 60, LR: 7.610619469026549e-05, Loss: 1.2034482955932617, Time:  0.36 hours
  Step: 80, LR: 6.725663716814161e-05, Loss: 1.1980280876159668, Time:  0.47 hours
  Step: 100, LR: 5.8407079646017705e-05, Loss: 1.2226451873779296, Time:  0.58 hours
evaluating mmlu
996/ 1531 inputs processed
baseline average mmlu val accuracy: 0.3481, time  0.60 min
evaluating mmlu
996/ 1531 inputs processed
baseline average mmlu test accuracy: 0.3553
CUDA mem allocated: 27.416858196258545 GB
CUDA mem reserved: 31.802734375 GB
before after evaluate
tot iter: 14847
  Step: 120, LR: 4.955752212389381e-05, Loss: 0.24364397525787354, Time:  0.02 hours
  Step: 140, LR: 4.0707964601769914e-05, Loss: 1.173271942138672, Time:  0.14 hours
evaluating mmlu
996/ 1531 inputs processed
baseline average mmlu val accuracy: 0.3586, time  0.60 min
  Step: 160, LR: 3.185840707964602e-05, Loss: 1.1579357147216798, Time:  0.26 hours
  Step: 180, LR: 2.3008849557522124e-05, Loss: 1.1609379768371582, Time:  0.37 hours
  Step: 200, LR: 1.415929203539823e-05, Loss: 1.1422094345092773, Time:  0.49 hours
evaluating mmlu
996/ 1531 inputs processed
baseline average mmlu val accuracy: 0.3599, time  0.60 min
  Step: 220, LR: 5.3097345132743365e-06, Loss: 1.1842758178710937, Time:  0.62 hours
tokenizer config file saved in ./results/7B_lora/tokenizer_config.json
Special tokens file saved in ./results/7B_lora/special_tokens_map.json
added tokens file saved in ./results/7B_lora/added_tokens.json
/orfeo/cephfs/scratch/area/ddoimo/open/open-instruct/env_amd/lib/python3.11/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /orfeo/cephfs/scratch/area/ddoimo/llama/llama_v1/models_hf/7B - will assume that the vocabulary was not modified.
  warnings.warn(
