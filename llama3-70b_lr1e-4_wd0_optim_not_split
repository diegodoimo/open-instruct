[2024-06-17 09:39:52,079] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
06/17/2024 09:42:44 - INFO - __main__ - Distributed environment: DistributedType.FSDP  Backend: nccl
Num processes: 8
Process index: 1
Local process index: 1
Device: cuda:1

Mixed precision type: bf16

06/17/2024 09:42:44 - INFO - __main__ - Distributed environment: DistributedType.FSDP  Backend: nccl
Num processes: 8
Process index: 2
Local process index: 2
Device: cuda:2

Mixed precision type: bf16

06/17/2024 09:42:44 - INFO - __main__ - Distributed environment: DistributedType.FSDP  Backend: nccl
Num processes: 8
Process index: 5
Local process index: 5
Device: cuda:5

Mixed precision type: bf16

06/17/2024 09:42:45 - INFO - __main__ - Distributed environment: DistributedType.FSDP  Backend: nccl
Num processes: 8
Process index: 0
Local process index: 0
Device: cuda:0

Mixed precision type: bf16

06/17/2024 09:42:45 - INFO - __main__ - Distributed environment: DistributedType.FSDP  Backend: nccl
Num processes: 8
Process index: 6
Local process index: 6
Device: cuda:6

Mixed precision type: bf16

06/17/2024 09:42:45 - INFO - __main__ - Distributed environment: DistributedType.FSDP  Backend: nccl
Num processes: 8
Process index: 7
Local process index: 7
Device: cuda:7

Mixed precision type: bf16

06/17/2024 09:42:45 - INFO - __main__ - Distributed environment: DistributedType.FSDP  Backend: nccl
Num processes: 8
Process index: 3
Local process index: 3
Device: cuda:3

Mixed precision type: bf16

06/17/2024 09:42:45 - INFO - __main__ - Distributed environment: DistributedType.FSDP  Backend: nccl
Num processes: 8
Process index: 4
Local process index: 4
Device: cuda:4

Mixed precision type: bf16

loading configuration file /orfeo/cephfs/scratch/area/ddoimo/models/llama_v3/models_hf/llama-3-70b/config.json
Model config LlamaConfig {
  "_name_or_path": "/orfeo/cephfs/scratch/area/ddoimo/models/llama_v3/models_hf/llama-3-70b",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "hidden_act": "silu",
  "hidden_size": 8192,
  "initializer_range": 0.02,
  "intermediate_size": 28672,
  "max_position_embeddings": 8192,
  "model_type": "llama",
  "num_attention_heads": 64,
  "num_hidden_layers": 80,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.40.0",
  "use_cache": true,
  "vocab_size": 128256
}

model_loading started. 


loading weights file /orfeo/cephfs/scratch/area/ddoimo/models/llama_v3/models_hf/llama-3-70b/model.safetensors.index.json
Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.
The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation="flash_attention_2"` instead.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Generate config GenerationConfig {
  "bos_token_id": 128000,
  "eos_token_id": 128001
}

Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]Loading checkpoint shards:   3%|▎         | 1/30 [00:01<00:50,  1.73s/it]Loading checkpoint shards:   7%|▋         | 2/30 [00:04<01:04,  2.32s/it]Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]Loading checkpoint shards:  10%|█         | 3/30 [00:07<01:07,  2.51s/it]Loading checkpoint shards:   3%|▎         | 1/30 [00:00<00:20,  1.41it/s]Loading checkpoint shards:   7%|▋         | 2/30 [00:01<00:15,  1.77it/s]Loading checkpoint shards:  10%|█         | 3/30 [00:01<00:11,  2.32it/s]Loading checkpoint shards:  13%|█▎        | 4/30 [00:09<01:04,  2.49s/it]Loading checkpoint shards:  13%|█▎        | 4/30 [00:03<00:23,  1.10it/s]Loading checkpoint shards:  17%|█▋        | 5/30 [00:05<00:40,  1.61s/it]Loading checkpoint shards:  17%|█▋        | 5/30 [00:12<01:07,  2.69s/it]Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]Loading checkpoint shards:   3%|▎         | 1/30 [00:00<00:09,  2.95it/s]Loading checkpoint shards:   3%|▎         | 1/30 [00:00<00:15,  1.88it/s]Loading checkpoint shards:   7%|▋         | 2/30 [00:00<00:08,  3.27it/s]Loading checkpoint shards:   7%|▋         | 2/30 [00:00<00:11,  2.49it/s]Loading checkpoint shards:  10%|█         | 3/30 [00:00<00:06,  4.04it/s]Loading checkpoint shards:  13%|█▎        | 4/30 [00:00<00:05,  4.39it/s]Loading checkpoint shards:  10%|█         | 3/30 [00:01<00:09,  2.82it/s]Loading checkpoint shards:  13%|█▎        | 4/30 [00:01<00:08,  3.08it/s]Loading checkpoint shards:  17%|█▋        | 5/30 [00:01<00:06,  3.82it/s]Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 5/30 [00:01<00:07,  3.31it/s]Loading checkpoint shards:   3%|▎         | 1/30 [00:00<00:15,  1.82it/s]Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]Loading checkpoint shards:  20%|██        | 6/30 [00:02<00:11,  2.04it/s]Loading checkpoint shards:  20%|██        | 6/30 [00:15<01:02,  2.60s/it]Loading checkpoint shards:   7%|▋         | 2/30 [00:00<00:10,  2.70it/s]Loading checkpoint shards:  20%|██        | 6/30 [00:08<00:46,  1.96s/it]Loading checkpoint shards:  20%|██        | 6/30 [00:02<00:11,  2.09it/s]Loading checkpoint shards:   3%|▎         | 1/30 [00:00<00:05,  5.08it/s]Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]Loading checkpoint shards:  10%|█         | 3/30 [00:01<00:08,  3.31it/s]Loading checkpoint shards:   7%|▋         | 2/30 [00:00<00:04,  5.89it/s]Loading checkpoint shards:   3%|▎         | 1/30 [00:00<00:04,  5.87it/s]Loading checkpoint shards:  13%|█▎        | 4/30 [00:01<00:06,  3.74it/s]Loading checkpoint shards:  10%|█         | 3/30 [00:00<00:04,  6.66it/s]Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]Loading checkpoint shards:   7%|▋         | 2/30 [00:00<00:03,  7.20it/s]Loading checkpoint shards:  13%|█▎        | 4/30 [00:00<00:03,  7.33it/s]Loading checkpoint shards:  17%|█▋        | 5/30 [00:01<00:05,  4.20it/s]Loading checkpoint shards:  10%|█         | 3/30 [00:00<00:03,  7.71it/s]Loading checkpoint shards:  17%|█▋        | 5/30 [00:00<00:03,  7.77it/s]Loading checkpoint shards:   3%|▎         | 1/30 [00:00<00:06,  4.18it/s]Loading checkpoint shards:  13%|█▎        | 4/30 [00:00<00:03,  8.02it/s]Loading checkpoint shards:  20%|██        | 6/30 [00:00<00:02,  8.09it/s]Loading checkpoint shards:  20%|██        | 6/30 [00:01<00:05,  4.51it/s]Loading checkpoint shards:   7%|▋         | 2/30 [00:00<00:04,  6.01it/s]Loading checkpoint shards:  17%|█▋        | 5/30 [00:00<00:03,  8.27it/s]Loading checkpoint shards:  20%|██        | 6/30 [00:00<00:02,  8.47it/s]Loading checkpoint shards:  10%|█         | 3/30 [00:00<00:03,  6.98it/s]Loading checkpoint shards:  13%|█▎        | 4/30 [00:00<00:03,  7.56it/s]Loading checkpoint shards:  17%|█▋        | 5/30 [00:00<00:03,  7.92it/s]Loading checkpoint shards:  20%|██        | 6/30 [00:00<00:02,  8.15it/s]Loading checkpoint shards:  23%|██▎       | 7/30 [00:02<00:11,  2.07it/s]Loading checkpoint shards:  23%|██▎       | 7/30 [00:17<00:59,  2.59s/it]Loading checkpoint shards:  23%|██▎       | 7/30 [00:02<00:15,  1.53it/s]Loading checkpoint shards:  23%|██▎       | 7/30 [00:04<00:26,  1.17s/it]Loading checkpoint shards:  23%|██▎       | 7/30 [00:02<00:13,  1.67it/s]Loading checkpoint shards:  23%|██▎       | 7/30 [00:11<00:49,  2.15s/it]Loading checkpoint shards:  23%|██▎       | 7/30 [00:03<00:16,  1.36it/s]Loading checkpoint shards:  23%|██▎       | 7/30 [00:05<00:26,  1.14s/it]Loading checkpoint shards:  27%|██▋       | 8/30 [00:04<00:26,  1.19s/it]Loading checkpoint shards:  27%|██▋       | 8/30 [00:20<00:57,  2.62s/it]Loading checkpoint shards:  27%|██▋       | 8/30 [00:05<00:28,  1.30s/it]Loading checkpoint shards:  27%|██▋       | 8/30 [00:05<00:27,  1.26s/it]Loading checkpoint shards:  27%|██▋       | 8/30 [00:07<00:36,  1.65s/it]Loading checkpoint shards:  27%|██▋       | 8/30 [00:13<00:51,  2.32s/it]Loading checkpoint shards:  27%|██▋       | 8/30 [00:06<00:29,  1.36s/it]Loading checkpoint shards:  27%|██▋       | 8/30 [00:07<00:35,  1.64s/it]Loading checkpoint shards:  30%|███       | 9/30 [00:06<00:30,  1.47s/it]Loading checkpoint shards:  30%|███       | 9/30 [00:22<00:51,  2.45s/it]Loading checkpoint shards:  30%|███       | 9/30 [00:07<00:32,  1.55s/it]Loading checkpoint shards:  30%|███       | 9/30 [00:07<00:31,  1.52s/it]Loading checkpoint shards:  30%|███       | 9/30 [00:09<00:37,  1.79s/it]Loading checkpoint shards:  30%|███       | 9/30 [00:15<00:47,  2.25s/it]Loading checkpoint shards:  30%|███       | 9/30 [00:08<00:33,  1.59s/it]Loading checkpoint shards:  30%|███       | 9/30 [00:09<00:37,  1.78s/it]Loading checkpoint shards:  33%|███▎      | 10/30 [00:09<00:37,  1.87s/it]Loading checkpoint shards:  33%|███▎      | 10/30 [00:25<00:50,  2.55s/it]Loading checkpoint shards:  33%|███▎      | 10/30 [00:10<00:38,  1.92s/it]Loading checkpoint shards:  33%|███▎      | 10/30 [00:09<00:38,  1.90s/it]Loading checkpoint shards:  33%|███▎      | 10/30 [00:12<00:41,  2.09s/it]Loading checkpoint shards:  33%|███▎      | 10/30 [00:18<00:48,  2.41s/it]Loading checkpoint shards:  33%|███▎      | 10/30 [00:10<00:38,  1.95s/it]Loading checkpoint shards:  33%|███▎      | 10/30 [00:12<00:41,  2.08s/it]Loading checkpoint shards:  37%|███▋      | 11/30 [00:11<00:35,  1.86s/it]Loading checkpoint shards:  37%|███▋      | 11/30 [00:27<00:44,  2.33s/it]Loading checkpoint shards:  37%|███▋      | 11/30 [00:11<00:35,  1.89s/it]Loading checkpoint shards:  37%|███▋      | 11/30 [00:11<00:35,  1.88s/it]Loading checkpoint shards:  37%|███▋      | 11/30 [00:14<00:38,  2.01s/it]Loading checkpoint shards:  37%|███▋      | 11/30 [00:20<00:42,  2.23s/it]Loading checkpoint shards:  37%|███▋      | 11/30 [00:12<00:36,  1.91s/it]Loading checkpoint shards:  37%|███▋      | 11/30 [00:14<00:38,  2.00s/it]Loading checkpoint shards:  40%|████      | 12/30 [00:14<00:38,  2.13s/it]Loading checkpoint shards:  40%|████      | 12/30 [00:29<00:44,  2.46s/it]Loading checkpoint shards:  40%|████      | 12/30 [00:14<00:38,  2.16s/it]Loading checkpoint shards:  40%|████      | 12/30 [00:14<00:38,  2.15s/it]Loading checkpoint shards:  40%|████      | 12/30 [00:16<00:40,  2.24s/it]Loading checkpoint shards:  40%|████      | 12/30 [00:23<00:43,  2.39s/it]Loading checkpoint shards:  40%|████      | 12/30 [00:15<00:39,  2.17s/it]Loading checkpoint shards:  40%|████      | 12/30 [00:17<00:40,  2.23s/it]Loading checkpoint shards:  43%|████▎     | 13/30 [00:17<00:40,  2.40s/it]Loading checkpoint shards:  43%|████▎     | 13/30 [00:32<00:44,  2.63s/it]Loading checkpoint shards:  43%|████▎     | 13/30 [00:17<00:41,  2.42s/it]Loading checkpoint shards:  43%|████▎     | 13/30 [00:17<00:41,  2.41s/it]Loading checkpoint shards:  43%|████▎     | 13/30 [00:19<00:42,  2.47s/it]Loading checkpoint shards:  43%|████▎     | 13/30 [00:26<00:43,  2.58s/it]Loading checkpoint shards:  43%|████▎     | 13/30 [00:18<00:41,  2.43s/it]Loading checkpoint shards:  43%|████▎     | 13/30 [00:20<00:42,  2.47s/it]Loading checkpoint shards:  47%|████▋     | 14/30 [00:19<00:36,  2.26s/it]Loading checkpoint shards:  47%|████▋     | 14/30 [00:34<00:38,  2.42s/it]Loading checkpoint shards:  47%|████▋     | 14/30 [00:19<00:36,  2.27s/it]Loading checkpoint shards:  47%|████▋     | 14/30 [00:19<00:36,  2.26s/it]Loading checkpoint shards:  47%|████▋     | 14/30 [00:21<00:36,  2.31s/it]Loading checkpoint shards:  47%|████▋     | 14/30 [00:28<00:38,  2.38s/it]Loading checkpoint shards:  47%|████▋     | 14/30 [00:20<00:36,  2.28s/it]Loading checkpoint shards:  47%|████▋     | 14/30 [00:22<00:36,  2.31s/it]Loading checkpoint shards:  50%|█████     | 15/30 [00:21<00:36,  2.41s/it]Loading checkpoint shards:  50%|█████     | 15/30 [00:22<00:36,  2.42s/it]Loading checkpoint shards:  50%|█████     | 15/30 [00:22<00:36,  2.41s/it]Loading checkpoint shards:  50%|█████     | 15/30 [00:37<00:37,  2.52s/it]Loading checkpoint shards:  50%|█████     | 15/30 [00:24<00:36,  2.44s/it]Loading checkpoint shards:  50%|█████     | 15/30 [00:30<00:37,  2.49s/it]Loading checkpoint shards:  50%|█████     | 15/30 [00:23<00:36,  2.42s/it]Loading checkpoint shards:  50%|█████     | 15/30 [00:24<00:36,  2.44s/it]Loading checkpoint shards:  53%|█████▎    | 16/30 [00:23<00:30,  2.17s/it]Loading checkpoint shards:  53%|█████▎    | 16/30 [00:39<00:31,  2.25s/it]Loading checkpoint shards:  53%|█████▎    | 16/30 [00:24<00:30,  2.18s/it]Loading checkpoint shards:  53%|█████▎    | 16/30 [00:23<00:30,  2.18s/it]Loading checkpoint shards:  53%|█████▎    | 16/30 [00:26<00:30,  2.20s/it]Loading checkpoint shards:  53%|█████▎    | 16/30 [00:32<00:31,  2.23s/it]Loading checkpoint shards:  53%|█████▎    | 16/30 [00:24<00:30,  2.18s/it]Loading checkpoint shards:  53%|█████▎    | 16/30 [00:26<00:30,  2.20s/it]Loading checkpoint shards:  57%|█████▋    | 17/30 [00:25<00:29,  2.25s/it]Loading checkpoint shards:  57%|█████▋    | 17/30 [00:41<00:29,  2.31s/it]Loading checkpoint shards:  57%|█████▋    | 17/30 [00:26<00:29,  2.26s/it]Loading checkpoint shards:  57%|█████▋    | 17/30 [00:26<00:29,  2.26s/it]Loading checkpoint shards:  57%|█████▋    | 17/30 [00:28<00:29,  2.27s/it]Loading checkpoint shards:  57%|█████▋    | 17/30 [00:35<00:29,  2.30s/it]Loading checkpoint shards:  57%|█████▋    | 17/30 [00:27<00:29,  2.26s/it]Loading checkpoint shards:  57%|█████▋    | 17/30 [00:28<00:29,  2.27s/it]Loading checkpoint shards:  60%|██████    | 18/30 [00:27<00:26,  2.17s/it]Loading checkpoint shards:  60%|██████    | 18/30 [00:43<00:26,  2.21s/it]Loading checkpoint shards:  60%|██████    | 18/30 [00:28<00:26,  2.17s/it]Loading checkpoint shards:  60%|██████    | 18/30 [00:28<00:26,  2.17s/it]Loading checkpoint shards:  60%|██████    | 18/30 [00:30<00:26,  2.18s/it]Loading checkpoint shards:  60%|██████    | 18/30 [00:36<00:26,  2.20s/it]Loading checkpoint shards:  60%|██████    | 18/30 [00:29<00:26,  2.18s/it]Loading checkpoint shards:  60%|██████    | 18/30 [00:30<00:26,  2.18s/it]Loading checkpoint shards:  63%|██████▎   | 19/30 [00:30<00:25,  2.32s/it]Loading checkpoint shards:  63%|██████▎   | 19/30 [00:46<00:25,  2.35s/it]Loading checkpoint shards:  63%|██████▎   | 19/30 [00:31<00:25,  2.33s/it]Loading checkpoint shards:  63%|██████▎   | 19/30 [00:30<00:25,  2.32s/it]Loading checkpoint shards:  63%|██████▎   | 19/30 [00:33<00:25,  2.33s/it]Loading checkpoint shards:  63%|██████▎   | 19/30 [00:39<00:25,  2.34s/it]Loading checkpoint shards:  63%|██████▎   | 19/30 [00:31<00:25,  2.33s/it]Loading checkpoint shards:  63%|██████▎   | 19/30 [00:33<00:25,  2.33s/it]Loading checkpoint shards:  67%|██████▋   | 20/30 [00:33<00:24,  2.40s/it]Loading checkpoint shards:  67%|██████▋   | 20/30 [00:48<00:24,  2.42s/it]Loading checkpoint shards:  67%|██████▋   | 20/30 [00:33<00:24,  2.41s/it]Loading checkpoint shards:  67%|██████▋   | 20/30 [00:33<00:24,  2.41s/it]Loading checkpoint shards:  67%|██████▋   | 20/30 [00:35<00:24,  2.41s/it]Loading checkpoint shards:  67%|██████▋   | 20/30 [00:42<00:24,  2.42s/it]Loading checkpoint shards:  67%|██████▋   | 20/30 [00:34<00:24,  2.41s/it]Loading checkpoint shards:  67%|██████▋   | 20/30 [00:36<00:24,  2.41s/it]Loading checkpoint shards:  70%|███████   | 21/30 [00:35<00:21,  2.44s/it]Loading checkpoint shards:  70%|███████   | 21/30 [00:51<00:22,  2.46s/it]Loading checkpoint shards:  70%|███████   | 21/30 [00:36<00:21,  2.44s/it]Loading checkpoint shards:  70%|███████   | 21/30 [00:36<00:21,  2.44s/it]Loading checkpoint shards:  70%|███████   | 21/30 [00:38<00:22,  2.45s/it]Loading checkpoint shards:  70%|███████   | 21/30 [00:44<00:22,  2.45s/it]Loading checkpoint shards:  70%|███████   | 21/30 [00:37<00:21,  2.44s/it]Loading checkpoint shards:  70%|███████   | 21/30 [00:38<00:22,  2.45s/it]Loading checkpoint shards:  73%|███████▎  | 22/30 [00:37<00:18,  2.33s/it]Loading checkpoint shards:  73%|███████▎  | 22/30 [00:53<00:18,  2.34s/it]Loading checkpoint shards:  73%|███████▎  | 22/30 [00:38<00:18,  2.33s/it]Loading checkpoint shards:  73%|███████▎  | 22/30 [00:38<00:18,  2.33s/it]Loading checkpoint shards:  73%|███████▎  | 22/30 [00:40<00:18,  2.33s/it]Loading checkpoint shards:  73%|███████▎  | 22/30 [00:46<00:18,  2.34s/it]Loading checkpoint shards:  73%|███████▎  | 22/30 [00:39<00:18,  2.33s/it]Loading checkpoint shards:  73%|███████▎  | 22/30 [00:40<00:18,  2.34s/it]Loading checkpoint shards:  77%|███████▋  | 23/30 [00:39<00:15,  2.20s/it]Loading checkpoint shards:  77%|███████▋  | 23/30 [00:55<00:15,  2.21s/it]Loading checkpoint shards:  77%|███████▋  | 23/30 [00:40<00:15,  2.20s/it]Loading checkpoint shards:  77%|███████▋  | 23/30 [00:39<00:15,  2.20s/it]Loading checkpoint shards:  77%|███████▋  | 23/30 [00:42<00:15,  2.20s/it]Loading checkpoint shards:  77%|███████▋  | 23/30 [00:40<00:15,  2.19s/it]Loading checkpoint shards:  77%|███████▋  | 23/30 [00:48<00:15,  2.21s/it]Loading checkpoint shards:  77%|███████▋  | 23/30 [00:42<00:15,  2.22s/it]Loading checkpoint shards:  80%|████████  | 24/30 [00:42<00:13,  2.24s/it]Loading checkpoint shards:  80%|████████  | 24/30 [00:43<00:13,  2.22s/it]Loading checkpoint shards:  80%|████████  | 24/30 [00:57<00:13,  2.24s/it]Loading checkpoint shards:  80%|████████  | 24/30 [00:42<00:13,  2.24s/it]Loading checkpoint shards:  80%|████████  | 24/30 [00:42<00:13,  2.24s/it]Loading checkpoint shards:  80%|████████  | 24/30 [00:44<00:13,  2.24s/it]Loading checkpoint shards:  80%|████████  | 24/30 [00:51<00:13,  2.24s/it]Loading checkpoint shards:  80%|████████  | 24/30 [00:44<00:13,  2.23s/it]Loading checkpoint shards:  83%|████████▎ | 25/30 [00:44<00:11,  2.29s/it]Loading checkpoint shards:  83%|████████▎ | 25/30 [00:45<00:11,  2.28s/it]Loading checkpoint shards:  83%|████████▎ | 25/30 [01:00<00:11,  2.30s/it]Loading checkpoint shards:  83%|████████▎ | 25/30 [00:44<00:11,  2.29s/it]Loading checkpoint shards:  83%|████████▎ | 25/30 [00:44<00:11,  2.29s/it]Loading checkpoint shards:  83%|████████▎ | 25/30 [00:47<00:11,  2.29s/it]Loading checkpoint shards:  83%|████████▎ | 25/30 [00:53<00:11,  2.29s/it]Loading checkpoint shards:  83%|████████▎ | 25/30 [00:47<00:11,  2.29s/it]Loading checkpoint shards:  87%|████████▋ | 26/30 [00:47<00:09,  2.48s/it]Loading checkpoint shards:  87%|████████▋ | 26/30 [00:48<00:09,  2.47s/it]Loading checkpoint shards:  87%|████████▋ | 26/30 [01:02<00:09,  2.48s/it]Loading checkpoint shards:  87%|████████▋ | 26/30 [00:47<00:09,  2.48s/it]Loading checkpoint shards:  87%|████████▋ | 26/30 [00:47<00:09,  2.48s/it]Loading checkpoint shards:  87%|████████▋ | 26/30 [00:50<00:09,  2.48s/it]Loading checkpoint shards:  87%|████████▋ | 26/30 [00:56<00:09,  2.48s/it]Loading checkpoint shards:  87%|████████▋ | 26/30 [00:50<00:09,  2.48s/it]Loading checkpoint shards:  90%|█████████ | 27/30 [00:49<00:07,  2.46s/it]Loading checkpoint shards:  90%|█████████ | 27/30 [00:51<00:07,  2.46s/it]Loading checkpoint shards:  90%|█████████ | 27/30 [01:05<00:07,  2.46s/it]Loading checkpoint shards:  90%|█████████ | 27/30 [00:50<00:07,  2.46s/it]Loading checkpoint shards:  90%|█████████ | 27/30 [00:50<00:07,  2.46s/it]Loading checkpoint shards:  90%|█████████ | 27/30 [00:52<00:07,  2.46s/it]Loading checkpoint shards:  90%|█████████ | 27/30 [00:58<00:07,  2.46s/it]Loading checkpoint shards:  90%|█████████ | 27/30 [00:52<00:07,  2.46s/it]Loading checkpoint shards:  93%|█████████▎| 28/30 [00:51<00:04,  2.38s/it]Loading checkpoint shards:  93%|█████████▎| 28/30 [00:53<00:04,  2.38s/it]Loading checkpoint shards:  93%|█████████▎| 28/30 [01:07<00:04,  2.39s/it]Loading checkpoint shards:  93%|█████████▎| 28/30 [00:52<00:04,  2.38s/it]Loading checkpoint shards:  93%|█████████▎| 28/30 [00:54<00:04,  2.39s/it]Loading checkpoint shards:  93%|█████████▎| 28/30 [00:52<00:04,  2.39s/it]Loading checkpoint shards:  93%|█████████▎| 28/30 [01:01<00:04,  2.38s/it]Loading checkpoint shards:  93%|█████████▎| 28/30 [00:54<00:04,  2.38s/it]Loading checkpoint shards:  97%|█████████▋| 29/30 [00:54<00:02,  2.45s/it]Loading checkpoint shards:  97%|█████████▋| 29/30 [00:55<00:02,  2.45s/it]Loading checkpoint shards:  97%|█████████▋| 29/30 [01:10<00:02,  2.45s/it]Loading checkpoint shards:  97%|█████████▋| 29/30 [00:55<00:02,  2.45s/it]Loading checkpoint shards:  97%|█████████▋| 29/30 [00:57<00:02,  2.45s/it]Loading checkpoint shards:  97%|█████████▋| 29/30 [00:54<00:02,  2.45s/it]Loading checkpoint shards:  97%|█████████▋| 29/30 [01:03<00:02,  2.45s/it]Loading checkpoint shards:  97%|█████████▋| 29/30 [00:57<00:02,  2.45s/it]Loading checkpoint shards: 100%|██████████| 30/30 [00:54<00:00,  1.77s/it]Loading checkpoint shards: 100%|██████████| 30/30 [00:54<00:00,  1.83s/it]
Loading checkpoint shards: 100%|██████████| 30/30 [00:56<00:00,  1.77s/it]Loading checkpoint shards: 100%|██████████| 30/30 [00:56<00:00,  1.87s/it]
Loading checkpoint shards: 100%|██████████| 30/30 [01:10<00:00,  1.77s/it]Loading checkpoint shards: 100%|██████████| 30/30 [01:10<00:00,  2.35s/it]
Loading checkpoint shards: 100%|██████████| 30/30 [00:55<00:00,  1.77s/it]Loading checkpoint shards: 100%|██████████| 30/30 [00:55<00:00,  1.84s/it]
Loading checkpoint shards: 100%|██████████| 30/30 [00:57<00:00,  1.77s/it]Loading checkpoint shards: 100%|██████████| 30/30 [00:57<00:00,  1.92s/it]
Loading checkpoint shards: 100%|██████████| 30/30 [00:55<00:00,  1.77s/it]Loading checkpoint shards: 100%|██████████| 30/30 [00:55<00:00,  1.83s/it]
Loading checkpoint shards: 100%|██████████| 30/30 [01:03<00:00,  1.77s/it]Loading checkpoint shards: 100%|██████████| 30/30 [01:03<00:00,  2.13s/it]
Loading checkpoint shards: 100%|██████████| 30/30 [00:57<00:00,  1.77s/it]Loading checkpoint shards: 100%|██████████| 30/30 [00:57<00:00,  1.92s/it]
All model checkpoint weights were used when initializing LlamaForCausalLM.

All the weights of LlamaForCausalLM were initialized from the model checkpoint at /orfeo/cephfs/scratch/area/ddoimo/models/llama_v3/models_hf/llama-3-70b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
loading configuration file /orfeo/cephfs/scratch/area/ddoimo/models/llama_v3/models_hf/llama-3-70b/generation_config.json
Generate config GenerationConfig {
  "bos_token_id": 128000,
  "eos_token_id": 128001
}

model loading finished. 


Initializing LORA model...
trainable params: 828,375,040 || all params: 71,382,081,536 || trainable%: 1.160480364504679
loading file tokenizer.json
loading file added_tokens.json
loading file special_tokens_map.json
loading file tokenizer_config.json
max_seq_len:  2048
loading dataset
split: train
mode: dev+validation

Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
tokenization started
tokenization finished
num_samples = 1065
loading dataset
split: validation
mode: validation

tokenization started
tokenization finished
loading dataset
split: test
mode: test

tokenization started
tokenization finished
memory consumed before loading model
CUDA mem allocated: 9.5367431640625e-07 GB
CUDA mem reserved: 0.001953125 GB
memory consumed after loading model
CUDA mem allocated: 24.760630130767822 GB
CUDA mem reserved: 29.828125 GB
preparing activation checkpointing..
setup scheduler and optimizer..
measuring baselines..
measuring validation accuracy
06/17/2024 10:12:03 - INFO - __main__ - iter 0. mmlu val accuracy: macro 0.7682, micro 0.7421
start training
memory before train run
CUDA mem allocated: 24.760630130767822 GB
CUDA mem reserved: 29.828125 GB
06/17/2024 10:12:04 - INFO - __main__ - ***** Running training *****
06/17/2024 10:12:04 - INFO - __main__ -   Num examples = 1065
06/17/2024 10:12:04 - INFO - __main__ -   Num Epochs = 4
06/17/2024 10:12:04 - INFO - __main__ -   len_dataloader = 133
06/17/2024 10:12:04 - INFO - __main__ -   Instantaneous batch size per device = 1
06/17/2024 10:12:04 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 16
06/17/2024 10:12:04 - INFO - __main__ -   Gradient Accumulation steps = 2
06/17/2024 10:12:04 - INFO - __main__ -   Total optimization steps = 268
06/17/2024 10:12:04 - INFO - __main__ -   Warmup steps = 13
06/17/2024 10:12:04 - INFO - __main__ -   Log steps number = 21
log step: 13/260
LR: 0.0001, Loss: 0.2951688032883864,                             Time: 0.0 h  1.59 min
log step: 26/260
LR: 9.936649105790168e-05, Loss: 0.3410513217632587,                             Time: 0.0 h  3.06 min
measuring validation accuracy
06/17/2024 10:18:51 - INFO - __main__ - iter 27. mmlu val accuracy: macro 0.7965, micro 0.7565
CUDA mem allocated: 26.885592937469482 GB
CUDA mem reserved: 29.828125 GB
log step: 39/260
LR: 9.748217972977712e-05, Loss: 0.2991918233724741,                             Time: 0.0 h  4.55 min
log step: 52/260
LR: 9.439529745304307e-05, Loss: 0.2831099033355713,                             Time: 0.0 h  6.04 min
measuring validation accuracy
06/17/2024 10:25:35 - INFO - __main__ - iter 54. mmlu val accuracy: macro 0.8182, micro 0.7736
CUDA mem allocated: 27.354118824005127 GB
CUDA mem reserved: 30.248046875 GB
log step: 65/260
LR: 9.01848570570133e-05, Loss: 0.27705783110398513,                             Time: 0.0 h  7.53 min
processed  217.33 token/sec
measuring test accuracy
1000/ 14042 inputs processed
06/17/2024 11:00:11 - INFO - __main__ - mmlu test accuracy after epoch 0: macro 0.7753, micro 0.7658
CUDA mem allocated: 27.354118824005127 GB
CUDA mem reserved: 30.248046875 GB
/orfeo/cephfs/scratch/area/ddoimo/finetuning_llm/open-instruct/env_amd_new/lib/python3.11/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /orfeo/cephfs/scratch/area/ddoimo/models/llama_v3/models_hf/llama-3-70b - will assume that the vocabulary was not modified.
  warnings.warn(
log step: 78/260
LR: 8.495863032516487e-05, Loss: 0.24090013137230507,                             Time: 0.0 h  9.13 min
measuring validation accuracy
06/17/2024 11:07:17 - INFO - __main__ - iter 81. mmlu val accuracy: macro 0.8480, micro 0.8010
CUDA mem allocated: 27.354118824005127 GB
CUDA mem reserved: 32.33984375 GB
log step: 91/260
LR: 7.885038943398501e-05, Loss: 0.19894108405480018,                             Time: 0.0 h  10.65 min
log step: 104/260
LR: 7.201648287741528e-05, Loss: 0.11636985265291654,                             Time: 0.0 h  12.14 min
measuring validation accuracy
06/17/2024 11:14:03 - INFO - __main__ - iter 108. mmlu val accuracy: macro 0.8719, micro 0.8128
CUDA mem allocated: 27.354118824005127 GB
CUDA mem reserved: 32.33984375 GB
log step: 117/260
LR: 6.463183352062944e-05, Loss: 0.212999545610868,                             Time: 0.0 h  13.64 min
log step: 130/260
LR: 5.6885461218245075e-05, Loss: 0.09490432189061092,                             Time: 0.0 h  15.13 min
processed  973.13 token/sec
measuring test accuracy
1000/ 14042 inputs processed
06/17/2024 11:50:12 - INFO - __main__ - mmlu test accuracy after epoch 1: macro 0.7729, micro 0.7634
CUDA mem allocated: 27.354118824005127 GB
CUDA mem reserved: 32.33984375 GB
measuring validation accuracy
06/17/2024 11:55:58 - INFO - __main__ - iter 135. mmlu val accuracy: macro 0.8906, micro 0.8469
CUDA mem allocated: 27.354118824005127 GB
CUDA mem reserved: 32.359375 GB
log step: 143/260
LR: 4.897564460146958e-05, Loss: 0.1669632654923659,                             Time: 0.0 h  16.72 min
log step: 156/260
LR: 4.110484587462962e-05, Loss: 0.07830557456383339,                             Time: 0.0 h  18.22 min
measuring validation accuracy
06/17/2024 12:02:42 - INFO - __main__ - iter 162. mmlu val accuracy: macro 0.9114, micro 0.8514
CUDA mem allocated: 27.354118824005127 GB
CUDA mem reserved: 32.359375 GB
log step: 169/260
LR: 3.347452852762056e-05, Loss: 0.04420351523619432,                             Time: 0.0 h  19.71 min
log step: 182/260
LR: 2.628000061177079e-05, Loss: 0.039577823418837324,                             Time: 0.0 h  21.20 min
measuring validation accuracy
06/17/2024 12:09:27 - INFO - __main__ - iter 189. mmlu val accuracy: macro 0.9245, micro 0.8652
CUDA mem allocated: 27.354118824005127 GB
CUDA mem reserved: 32.359375 GB
log step: 195/260
LR: 1.970541557228705e-05, Loss: 0.05018669366836548,                             Time: 0.0 h  22.69 min
processed  5264.61 token/sec
measuring test accuracy
1000/ 14042 inputs processed
06/17/2024 12:43:57 - INFO - __main__ - mmlu test accuracy after epoch 2: macro 0.7787, micro 0.7713
CUDA mem allocated: 27.354118824005127 GB
CUDA mem reserved: 32.359375 GB
log step: 208/260
LR: 1.391905859757738e-05, Loss: 0.053744219816648044,                             Time: 0.0 h  24.30 min
measuring validation accuracy
06/17/2024 12:51:30 - INFO - __main__ - iter 216. mmlu val accuracy: macro 0.9277, micro 0.8685
CUDA mem allocated: 27.354118824005127 GB
CUDA mem reserved: 32.359375 GB
log step: 221/260
LR: 9.069039137567103e-06, Loss: 0.012424403658280006,                             Time: 0.0 h  25.81 min
log step: 234/260
LR: 5.279499846691252e-06, Loss: 0.014913212794523973,                             Time: 0.0 h  27.30 min
measuring validation accuracy
06/17/2024 12:58:14 - INFO - __main__ - iter 243. mmlu val accuracy: macro 0.9303, micro 0.8704
CUDA mem allocated: 27.354118824005127 GB
CUDA mem reserved: 32.359375 GB
log step: 247/260
LR: 2.6474389886752223e-06, Loss: 0.0013322798678508173,                             Time: 0.0 h  28.80 min
log step: 260/260
LR: 1.2402276378503107e-06, Loss: 0.044733134599832386,                             Time: 0.0 h  30.28 min
processed  31620.58 token/sec
measuring test accuracy
1000/ 14042 inputs processed
06/17/2024 13:34:07 - INFO - __main__ - mmlu test accuracy after epoch 3: macro 0.7775, micro 0.7704
CUDA mem allocated: 27.354118824005127 GB
CUDA mem reserved: 32.359375 GB
