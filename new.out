[2024-04-17 08:55:57,554] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/orfeo/cephfs/scratch/area/ddoimo/open/open-instruct/env_amd/lib/python3.11/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
[2024-04-17 08:57:33,151] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/orfeo/cephfs/scratch/area/ddoimo/open/open-instruct/env_amd/lib/python3.11/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/orfeo/cephfs/scratch/area/ddoimo/open/open-instruct/env_amd/lib/python3.11/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
04/17/2024 08:57:45 - INFO - __main__ - Distributed environment: DistributedType.NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: bf16

loading configuration file /orfeo/cephfs/scratch/area/ddoimo/llama/llama_v1/models_hf/7B/config.json
Model config LlamaConfig {
  "_name_or_path": "/orfeo/cephfs/scratch/area/ddoimo/llama/llama_v1/models_hf/7B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.35.0.dev0",
  "use_cache": true,
  "vocab_size": 32000
}

model_loading started. 


LlamaConfig {
  "_name_or_path": "/orfeo/cephfs/scratch/area/ddoimo/llama/llama_v1/models_hf/7B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.35.0.dev0",
  "use_cache": true,
  "vocab_size": 32000
}

loading weights file /orfeo/cephfs/scratch/area/ddoimo/llama/llama_v1/models_hf/7B/pytorch_model.bin.index.json
Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.
Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2
}

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:45<00:45, 45.26s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:00<00:00, 27.42s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:00<00:00, 30.09s/it]
All model checkpoint weights were used when initializing LlamaForCausalLM.

All the weights of LlamaForCausalLM were initialized from the model checkpoint at /orfeo/cephfs/scratch/area/ddoimo/llama/llama_v1/models_hf/7B.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
loading configuration file /orfeo/cephfs/scratch/area/ddoimo/llama/llama_v1/models_hf/7B/generation_config.json
Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2
}

model loading finished. 


04/17/2024 08:59:42 - INFO - __main__ - Initializing LORA model...
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3180681656919973
model loaded. 


loading file tokenizer.model
loading file added_tokens.json
loading file special_tokens_map.json
loading file tokenizer_config.json
loading file tokenizer.json
Adding <s> to the vocabulary
Adding </s> to the vocabulary
Adding <unk> to the vocabulary
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Assigning <s> to the bos_token key of the tokenizer
Assigning </s> to the eos_token key of the tokenizer
Assigning <unk> to the unk_token key of the tokenizer
Assigning <pad> to the pad_token key of the tokenizer
Adding <s> to the vocabulary
Adding </s> to the vocabulary
Adding <unk> to the vocabulary
Adding <pad> to the vocabulary
/orfeo/cephfs/scratch/area/ddoimo/open/open-instruct/env_amd/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by promote_options='default'.
  table = cls._concat_blocks(blocks, axis=0)
loading dataframes
/orfeo/cephfs/scratch/area/ddoimo/open/open-instruct/env_amd/lib/python3.11/site-packages/datasets/table.py:1395: FutureWarning: promote has been superseded by promote_options='default'.
  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]
/orfeo/cephfs/scratch/area/ddoimo/open/open-instruct/env_amd/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by promote_options='default'.
  table = cls._concat_blocks(blocks, axis=0)
loading dataframes
/orfeo/cephfs/scratch/area/ddoimo/open/open-instruct/env_amd/lib/python3.11/site-packages/datasets/table.py:1395: FutureWarning: promote has been superseded by promote_options='default'.
  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]
/orfeo/cephfs/scratch/area/ddoimo/open/open-instruct/env_amd/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by promote_options='default'.
  table = cls._concat_blocks(blocks, axis=0)
CUDA mem allocated: 12.880393981933594 GB
CUDA mem reserved: 12.892578125 GB
before train run
evaluating mmlu
996/ 14042 inputs processed
1996/ 14042 inputs processed
2996/ 14042 inputs processed
3996/ 14042 inputs processed
4996/ 14042 inputs processed
5996/ 14042 inputs processed
6996/ 14042 inputs processed
7996/ 14042 inputs processed
8996/ 14042 inputs processed
9996/ 14042 inputs processed
10996/ 14042 inputs processed
11996/ 14042 inputs processed
12996/ 14042 inputs processed
13996/ 14042 inputs processed
baseline average mmlu test accuracy: 0.3179
CUDA mem allocated: 21.356714725494385 GB
CUDA mem reserved: 25.248046875 GB
before after evaluate
  Step: 20, LR: 9.43231441048035e-05, Loss: 1.6640920639038086, Time:  0.12 hours
CUDA mem allocated: 27.406539916992188 GB
CUDA mem reserved: 33.609375 GB
  Step: 40, LR: 8.558951965065502e-05, Loss: 1.2421339988708495, Time:  0.24 hours
CUDA mem allocated: 27.406539916992188 GB
CUDA mem reserved: 33.609375 GB
evaluating mmlu
996/ 1531 inputs processed
baseline average mmlu val accuracy: 0.3233, time  0.59 min
  Step: 60, LR: 7.685589519650656e-05, Loss: 1.1659045219421387, Time:  0.37 hours
CUDA mem allocated: 27.406539916992188 GB
CUDA mem reserved: 33.609375 GB
  Step: 80, LR: 6.812227074235808e-05, Loss: 1.1235076904296875, Time:  0.49 hours
CUDA mem allocated: 27.406539916992188 GB
CUDA mem reserved: 38.1953125 GB
  Step: 100, LR: 5.9388646288209616e-05, Loss: 1.130209255218506, Time:  0.61 hours
CUDA mem allocated: 27.406539916992188 GB
CUDA mem reserved: 38.1953125 GB
evaluating mmlu
996/ 1531 inputs processed
baseline average mmlu val accuracy: 0.3325, time  0.59 min
  Step: 120, LR: 5.065502183406113e-05, Loss: 0.10987370014190674, Time:  0.01 hours
CUDA mem allocated: 27.406539916992188 GB
CUDA mem reserved: 38.1953125 GB
  Step: 140, LR: 4.192139737991266e-05, Loss: 1.1029083251953125, Time:  0.13 hours
CUDA mem allocated: 27.406539916992188 GB
CUDA mem reserved: 38.1953125 GB
evaluating mmlu
996/ 1531 inputs processed
baseline average mmlu val accuracy: 0.3285, time  0.59 min
  Step: 160, LR: 3.3187772925764197e-05, Loss: 1.1085869789123535, Time:  0.26 hours
CUDA mem allocated: 27.406539916992188 GB
CUDA mem reserved: 38.1953125 GB
  Step: 180, LR: 2.445414847161572e-05, Loss: 1.1040765762329101, Time:  0.38 hours
CUDA mem allocated: 27.406539916992188 GB
CUDA mem reserved: 38.1953125 GB
  Step: 200, LR: 1.572052401746725e-05, Loss: 1.096559429168701, Time:  0.50 hours
CUDA mem allocated: 27.406539916992188 GB
CUDA mem reserved: 38.1953125 GB
evaluating mmlu
996/ 1531 inputs processed
baseline average mmlu val accuracy: 0.3344, time  0.59 min
  Step: 220, LR: 6.986899563318778e-06, Loss: 1.1117966651916504, Time:  0.63 hours
CUDA mem allocated: 27.406539916992188 GB
CUDA mem reserved: 38.1953125 GB
evaluating mmlu
996/ 14042 inputs processed
1996/ 14042 inputs processed
2996/ 14042 inputs processed
3996/ 14042 inputs processed
4996/ 14042 inputs processed
5996/ 14042 inputs processed
6996/ 14042 inputs processed
7996/ 14042 inputs processed
8996/ 14042 inputs processed
9996/ 14042 inputs processed
10996/ 14042 inputs processed
11996/ 14042 inputs processed
12996/ 14042 inputs processed
13996/ 14042 inputs processed
baseline average mmlu test accuracy: 0.3301
CUDA mem allocated: 27.406539916992188 GB
CUDA mem reserved: 38.1953125 GB
before after evaluate
