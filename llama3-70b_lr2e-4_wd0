[2024-06-17 22:34:10,603] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
06/17/2024 22:35:30 - INFO - __main__ - Distributed environment: DistributedType.FSDP  Backend: nccl
Num processes: 8
Process index: 0
Local process index: 0
Device: cuda:0

Mixed precision type: bf16

06/17/2024 22:35:30 - INFO - __main__ - Distributed environment: DistributedType.FSDP  Backend: nccl
Num processes: 8
Process index: 1
Local process index: 1
Device: cuda:1

Mixed precision type: bf16

06/17/2024 22:35:30 - INFO - __main__ - Distributed environment: DistributedType.FSDP  Backend: nccl
Num processes: 8
Process index: 2
Local process index: 2
Device: cuda:2

Mixed precision type: bf16

06/17/2024 22:35:30 - INFO - __main__ - Distributed environment: DistributedType.FSDP  Backend: nccl
Num processes: 8
Process index: 3
Local process index: 3
Device: cuda:3

Mixed precision type: bf16

06/17/2024 22:35:30 - INFO - __main__ - Distributed environment: DistributedType.FSDP  Backend: nccl
Num processes: 8
Process index: 5
Local process index: 5
Device: cuda:5

Mixed precision type: bf16

06/17/2024 22:35:30 - INFO - __main__ - Distributed environment: DistributedType.FSDP  Backend: nccl
Num processes: 8
Process index: 6
Local process index: 6
Device: cuda:6

Mixed precision type: bf16

06/17/2024 22:35:30 - INFO - __main__ - Distributed environment: DistributedType.FSDP  Backend: nccl
Num processes: 8
Process index: 4
Local process index: 4
Device: cuda:4

Mixed precision type: bf16

06/17/2024 22:35:30 - INFO - __main__ - Distributed environment: DistributedType.FSDP  Backend: nccl
Num processes: 8
Process index: 7
Local process index: 7
Device: cuda:7

Mixed precision type: bf16

loading configuration file /orfeo/cephfs/scratch/area/ddoimo/models/llama_v3/models_hf/llama-3-70b/config.json
Model config LlamaConfig {
  "_name_or_path": "/orfeo/cephfs/scratch/area/ddoimo/models/llama_v3/models_hf/llama-3-70b",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "hidden_act": "silu",
  "hidden_size": 8192,
  "initializer_range": 0.02,
  "intermediate_size": 28672,
  "max_position_embeddings": 8192,
  "model_type": "llama",
  "num_attention_heads": 64,
  "num_hidden_layers": 80,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.40.0",
  "use_cache": true,
  "vocab_size": 128256
}

model_loading started. 


loading weights file /orfeo/cephfs/scratch/area/ddoimo/models/llama_v3/models_hf/llama-3-70b/model.safetensors.index.json
Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.
The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation="flash_attention_2"` instead.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Generate config GenerationConfig {
  "bos_token_id": 128000,
  "eos_token_id": 128001
}

Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]Loading checkpoint shards:   3%|▎         | 1/30 [00:01<00:48,  1.68s/it]Loading checkpoint shards:   3%|▎         | 1/30 [00:01<00:31,  1.07s/it]Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]Loading checkpoint shards:   3%|▎         | 1/30 [00:00<00:17,  1.67it/s]Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]Loading checkpoint shards:   3%|▎         | 1/30 [00:00<00:12,  2.32it/s]Loading checkpoint shards:   7%|▋         | 2/30 [00:04<01:04,  2.29s/it]Loading checkpoint shards:   7%|▋         | 2/30 [00:03<00:54,  1.94s/it]Loading checkpoint shards:   7%|▋         | 2/30 [00:01<00:22,  1.26it/s]Loading checkpoint shards:   7%|▋         | 2/30 [00:02<00:36,  1.29s/it]Loading checkpoint shards:  10%|█         | 3/30 [00:07<01:06,  2.45s/it]Loading checkpoint shards:  10%|█         | 3/30 [00:06<01:01,  2.28s/it]Loading checkpoint shards:  10%|█         | 3/30 [00:04<00:46,  1.72s/it]Loading checkpoint shards:  10%|█         | 3/30 [00:05<00:53,  1.99s/it]Loading checkpoint shards:  13%|█▎        | 4/30 [00:09<01:03,  2.46s/it]Loading checkpoint shards:  13%|█▎        | 4/30 [00:07<00:53,  2.06s/it]Loading checkpoint shards:  13%|█▎        | 4/30 [00:08<01:02,  2.39s/it]Loading checkpoint shards:  13%|█▎        | 4/30 [00:06<00:51,  1.97s/it]Loading checkpoint shards:  17%|█▋        | 5/30 [00:12<01:05,  2.62s/it]Loading checkpoint shards:  17%|█▋        | 5/30 [00:09<00:56,  2.24s/it]Loading checkpoint shards:  17%|█▋        | 5/30 [00:10<00:59,  2.38s/it]Loading checkpoint shards:  17%|█▋        | 5/30 [00:11<01:04,  2.59s/it]Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]Loading checkpoint shards:   3%|▎         | 1/30 [00:00<00:11,  2.60it/s]Loading checkpoint shards:   7%|▋         | 2/30 [00:00<00:07,  3.67it/s]Loading checkpoint shards:  10%|█         | 3/30 [00:00<00:05,  4.79it/s]Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]Loading checkpoint shards:  13%|█▎        | 4/30 [00:00<00:04,  5.64it/s]Loading checkpoint shards:   3%|▎         | 1/30 [00:00<00:05,  5.72it/s]Loading checkpoint shards:  17%|█▋        | 5/30 [00:00<00:03,  6.25it/s]Loading checkpoint shards:   7%|▋         | 2/30 [00:00<00:04,  5.90it/s]Loading checkpoint shards:  10%|█         | 3/30 [00:00<00:04,  6.53it/s]Loading checkpoint shards:  13%|█▎        | 4/30 [00:00<00:03,  6.89it/s]Loading checkpoint shards:  17%|█▋        | 5/30 [00:00<00:03,  7.12it/s]Loading checkpoint shards:  20%|██        | 6/30 [00:01<00:07,  3.30it/s]Loading checkpoint shards:  20%|██        | 6/30 [00:02<00:11,  2.00it/s]Loading checkpoint shards:  20%|██        | 6/30 [00:13<00:58,  2.44s/it]Loading checkpoint shards:  20%|██        | 6/30 [00:14<01:01,  2.54s/it]Loading checkpoint shards:  20%|██        | 6/30 [00:11<00:54,  2.28s/it]Loading checkpoint shards:  20%|██        | 6/30 [00:12<00:56,  2.37s/it]Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]Loading checkpoint shards:   3%|▎         | 1/30 [00:00<00:04,  6.25it/s]Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]Loading checkpoint shards:   7%|▋         | 2/30 [00:00<00:03,  7.38it/s]Loading checkpoint shards:   3%|▎         | 1/30 [00:00<00:04,  6.93it/s]Loading checkpoint shards:  10%|█         | 3/30 [00:00<00:03,  7.84it/s]Loading checkpoint shards:   7%|▋         | 2/30 [00:00<00:03,  7.85it/s]Loading checkpoint shards:  13%|█▎        | 4/30 [00:00<00:03,  8.10it/s]Loading checkpoint shards:  10%|█         | 3/30 [00:00<00:03,  8.18it/s]Loading checkpoint shards:  17%|█▋        | 5/30 [00:00<00:03,  8.27it/s]Loading checkpoint shards:  13%|█▎        | 4/30 [00:00<00:03,  8.36it/s]Loading checkpoint shards:  20%|██        | 6/30 [00:00<00:02,  8.36it/s]Loading checkpoint shards:  17%|█▋        | 5/30 [00:00<00:02,  8.49it/s]Loading checkpoint shards:  20%|██        | 6/30 [00:00<00:02,  8.63it/s]Loading checkpoint shards:  23%|██▎       | 7/30 [00:17<00:57,  2.48s/it]Loading checkpoint shards:  23%|██▎       | 7/30 [00:01<00:04,  4.90it/s]Loading checkpoint shards:  23%|██▎       | 7/30 [00:01<00:05,  3.94it/s]Loading checkpoint shards:  23%|██▎       | 7/30 [00:03<00:22,  1.01it/s]Loading checkpoint shards:  23%|██▎       | 7/30 [00:14<00:53,  2.31s/it]Loading checkpoint shards:  23%|██▎       | 7/30 [00:14<00:53,  2.34s/it]Loading checkpoint shards:  23%|██▎       | 7/30 [00:04<00:26,  1.16s/it]Loading checkpoint shards:  23%|██▎       | 7/30 [00:16<00:56,  2.48s/it]Loading checkpoint shards:  27%|██▋       | 8/30 [00:19<00:53,  2.42s/it]Loading checkpoint shards:  27%|██▋       | 8/30 [00:03<00:19,  1.15it/s]Loading checkpoint shards:  27%|██▋       | 8/30 [00:16<00:50,  2.30s/it]Loading checkpoint shards:  27%|██▋       | 8/30 [00:03<00:19,  1.11it/s]Loading checkpoint shards:  27%|██▋       | 8/30 [00:06<00:30,  1.40s/it]Loading checkpoint shards:  27%|██▋       | 8/30 [00:17<00:51,  2.32s/it]Loading checkpoint shards:  27%|██▋       | 8/30 [00:06<00:32,  1.50s/it]Loading checkpoint shards:  27%|██▋       | 8/30 [00:18<00:52,  2.39s/it]Loading checkpoint shards:  30%|███       | 9/30 [00:21<00:50,  2.42s/it]Loading checkpoint shards:  30%|███       | 9/30 [00:05<00:28,  1.38s/it]Loading checkpoint shards:  30%|███       | 9/30 [00:05<00:28,  1.36s/it]Loading checkpoint shards:  30%|███       | 9/30 [00:18<00:49,  2.34s/it]Loading checkpoint shards:  30%|███       | 9/30 [00:08<00:36,  1.72s/it]Loading checkpoint shards:  30%|███       | 9/30 [00:19<00:49,  2.36s/it]Loading checkpoint shards:  30%|███       | 9/30 [00:09<00:37,  1.79s/it]Loading checkpoint shards:  30%|███       | 9/30 [00:21<00:50,  2.40s/it]Loading checkpoint shards:  33%|███▎      | 10/30 [00:23<00:44,  2.23s/it]Loading checkpoint shards:  33%|███▎      | 10/30 [00:07<00:29,  1.49s/it]Loading checkpoint shards:  33%|███▎      | 10/30 [00:20<00:43,  2.17s/it]Loading checkpoint shards:  33%|███▎      | 10/30 [00:10<00:34,  1.74s/it]Loading checkpoint shards:  33%|███▎      | 10/30 [00:07<00:30,  1.50s/it]Loading checkpoint shards:  33%|███▎      | 10/30 [00:21<00:43,  2.18s/it]Loading checkpoint shards:  33%|███▎      | 10/30 [00:11<00:35,  1.79s/it]Loading checkpoint shards:  33%|███▎      | 10/30 [00:22<00:44,  2.22s/it]Loading checkpoint shards:  37%|███▋      | 11/30 [00:25<00:41,  2.20s/it]Loading checkpoint shards:  37%|███▋      | 11/30 [00:09<00:31,  1.68s/it]Loading checkpoint shards:  37%|███▋      | 11/30 [00:22<00:40,  2.16s/it]Loading checkpoint shards:  37%|███▋      | 11/30 [00:12<00:35,  1.86s/it]Loading checkpoint shards:  37%|███▋      | 11/30 [00:09<00:32,  1.69s/it]Loading checkpoint shards:  37%|███▋      | 11/30 [00:23<00:41,  2.16s/it]Loading checkpoint shards:  37%|███▋      | 11/30 [00:13<00:35,  1.89s/it]Loading checkpoint shards:  37%|███▋      | 11/30 [00:25<00:41,  2.19s/it]Loading checkpoint shards:  40%|████      | 12/30 [00:28<00:39,  2.21s/it]Loading checkpoint shards:  40%|████      | 12/30 [00:14<00:35,  1.97s/it]Loading checkpoint shards:  40%|████      | 12/30 [00:12<00:33,  1.86s/it]Loading checkpoint shards:  40%|████      | 12/30 [00:11<00:33,  1.85s/it]Loading checkpoint shards:  40%|████      | 12/30 [00:24<00:39,  2.18s/it]Loading checkpoint shards:  40%|████      | 12/30 [00:25<00:39,  2.18s/it]Loading checkpoint shards:  40%|████      | 12/30 [00:15<00:35,  1.98s/it]Loading checkpoint shards:  40%|████      | 12/30 [00:27<00:39,  2.20s/it]Loading checkpoint shards:  43%|████▎     | 13/30 [00:30<00:39,  2.31s/it]Loading checkpoint shards:  43%|████▎     | 13/30 [00:17<00:36,  2.14s/it]Loading checkpoint shards:  43%|████▎     | 13/30 [00:14<00:35,  2.06s/it]Loading checkpoint shards:  43%|████▎     | 13/30 [00:14<00:35,  2.06s/it]Loading checkpoint shards:  43%|████▎     | 13/30 [00:27<00:38,  2.29s/it]Loading checkpoint shards:  43%|████▎     | 13/30 [00:28<00:38,  2.29s/it]Loading checkpoint shards:  43%|████▎     | 13/30 [00:17<00:36,  2.15s/it]Loading checkpoint shards:  43%|████▎     | 13/30 [00:29<00:39,  2.30s/it]Loading checkpoint shards:  47%|████▋     | 14/30 [00:33<00:39,  2.47s/it]Loading checkpoint shards:  47%|████▋     | 14/30 [00:20<00:37,  2.36s/it]Loading checkpoint shards:  47%|████▋     | 14/30 [00:17<00:36,  2.30s/it]Loading checkpoint shards:  47%|████▋     | 14/30 [00:17<00:36,  2.31s/it]Loading checkpoint shards:  47%|████▋     | 14/30 [00:30<00:39,  2.46s/it]Loading checkpoint shards:  47%|████▋     | 14/30 [00:31<00:39,  2.46s/it]Loading checkpoint shards:  47%|████▋     | 14/30 [00:20<00:37,  2.36s/it]Loading checkpoint shards:  47%|████▋     | 14/30 [00:32<00:39,  2.47s/it]Loading checkpoint shards:  50%|█████     | 15/30 [00:35<00:37,  2.50s/it]Loading checkpoint shards:  50%|█████     | 15/30 [00:22<00:36,  2.42s/it]Loading checkpoint shards:  50%|█████     | 15/30 [00:20<00:35,  2.38s/it]Loading checkpoint shards:  50%|█████     | 15/30 [00:19<00:35,  2.38s/it]Loading checkpoint shards:  50%|█████     | 15/30 [00:32<00:37,  2.49s/it]Loading checkpoint shards:  50%|█████     | 15/30 [00:33<00:37,  2.49s/it]Loading checkpoint shards:  50%|█████     | 15/30 [00:23<00:36,  2.42s/it]Loading checkpoint shards:  50%|█████     | 15/30 [00:35<00:37,  2.50s/it]Loading checkpoint shards:  53%|█████▎    | 16/30 [00:38<00:35,  2.53s/it]Loading checkpoint shards:  53%|█████▎    | 16/30 [00:35<00:35,  2.52s/it]Loading checkpoint shards:  53%|█████▎    | 16/30 [00:22<00:34,  2.45s/it]Loading checkpoint shards:  53%|█████▎    | 16/30 [00:22<00:34,  2.45s/it]Loading checkpoint shards:  53%|█████▎    | 16/30 [00:25<00:34,  2.48s/it]Loading checkpoint shards:  53%|█████▎    | 16/30 [00:36<00:35,  2.53s/it]Loading checkpoint shards:  53%|█████▎    | 16/30 [00:25<00:34,  2.48s/it]Loading checkpoint shards:  53%|█████▎    | 16/30 [00:37<00:35,  2.52s/it]Loading checkpoint shards:  57%|█████▋    | 17/30 [00:41<00:33,  2.55s/it]Loading checkpoint shards:  57%|█████▋    | 17/30 [00:25<00:32,  2.49s/it]Loading checkpoint shards:  57%|█████▋    | 17/30 [00:25<00:32,  2.49s/it]Loading checkpoint shards:  57%|█████▋    | 17/30 [00:38<00:33,  2.55s/it]Loading checkpoint shards:  57%|█████▋    | 17/30 [00:40<00:32,  2.52s/it]Loading checkpoint shards:  57%|█████▋    | 17/30 [00:38<00:33,  2.55s/it]Loading checkpoint shards:  57%|█████▋    | 17/30 [00:27<00:32,  2.52s/it]Loading checkpoint shards:  57%|█████▋    | 17/30 [00:28<00:32,  2.51s/it]Loading checkpoint shards:  60%|██████    | 18/30 [00:44<00:34,  2.87s/it]Loading checkpoint shards:  60%|██████    | 18/30 [00:32<00:33,  2.82s/it]Loading checkpoint shards:  60%|██████    | 18/30 [00:28<00:33,  2.83s/it]Loading checkpoint shards:  60%|██████    | 18/30 [00:41<00:34,  2.86s/it]Loading checkpoint shards:  60%|██████    | 18/30 [00:28<00:33,  2.82s/it]Loading checkpoint shards:  60%|██████    | 18/30 [00:31<00:34,  2.84s/it]Loading checkpoint shards:  60%|██████    | 18/30 [00:42<00:34,  2.86s/it]Loading checkpoint shards:  60%|██████    | 18/30 [00:43<00:34,  2.85s/it]Loading checkpoint shards:  63%|██████▎   | 19/30 [00:34<00:29,  2.70s/it]Loading checkpoint shards:  63%|██████▎   | 19/30 [00:47<00:30,  2.73s/it]Loading checkpoint shards:  63%|██████▎   | 19/30 [00:46<00:29,  2.71s/it]Loading checkpoint shards:  63%|██████▎   | 19/30 [00:44<00:30,  2.73s/it]Loading checkpoint shards:  63%|██████▎   | 19/30 [00:33<00:29,  2.71s/it]Loading checkpoint shards:  63%|██████▎   | 19/30 [00:31<00:29,  2.70s/it]Loading checkpoint shards:  63%|██████▎   | 19/30 [00:31<00:29,  2.71s/it]Loading checkpoint shards:  63%|██████▎   | 19/30 [00:44<00:30,  2.73s/it]Loading checkpoint shards:  67%|██████▋   | 20/30 [00:37<00:27,  2.72s/it]Loading checkpoint shards:  67%|██████▋   | 20/30 [00:49<00:27,  2.74s/it]Loading checkpoint shards:  67%|██████▋   | 20/30 [00:49<00:27,  2.72s/it]Loading checkpoint shards:  67%|██████▋   | 20/30 [00:34<00:27,  2.72s/it]Loading checkpoint shards:  67%|██████▋   | 20/30 [00:36<00:27,  2.72s/it]Loading checkpoint shards:  67%|██████▋   | 20/30 [00:33<00:27,  2.72s/it]Loading checkpoint shards:  67%|██████▋   | 20/30 [00:46<00:27,  2.74s/it]Loading checkpoint shards:  67%|██████▋   | 20/30 [00:47<00:27,  2.74s/it]Loading checkpoint shards:  70%|███████   | 21/30 [00:40<00:25,  2.85s/it]Loading checkpoint shards:  70%|███████   | 21/30 [00:53<00:25,  2.86s/it]Loading checkpoint shards:  70%|███████   | 21/30 [00:52<00:25,  2.85s/it]Loading checkpoint shards:  70%|███████   | 21/30 [00:50<00:25,  2.86s/it]Loading checkpoint shards:  70%|███████   | 21/30 [00:39<00:25,  2.85s/it]Loading checkpoint shards:  70%|███████   | 21/30 [00:37<00:25,  2.85s/it]Loading checkpoint shards:  70%|███████   | 21/30 [00:37<00:25,  2.85s/it]Loading checkpoint shards:  70%|███████   | 21/30 [00:50<00:25,  2.86s/it]Loading checkpoint shards:  73%|███████▎  | 22/30 [00:42<00:20,  2.54s/it]Loading checkpoint shards:  73%|███████▎  | 22/30 [00:54<00:20,  2.55s/it]Loading checkpoint shards:  73%|███████▎  | 22/30 [00:54<00:20,  2.54s/it]Loading checkpoint shards:  73%|███████▎  | 22/30 [00:38<00:20,  2.54s/it]Loading checkpoint shards:  73%|███████▎  | 22/30 [00:39<00:20,  2.54s/it]Loading checkpoint shards:  73%|███████▎  | 22/30 [00:51<00:20,  2.55s/it]Loading checkpoint shards:  73%|███████▎  | 22/30 [00:41<00:20,  2.54s/it]Loading checkpoint shards:  73%|███████▎  | 22/30 [00:52<00:20,  2.55s/it]Loading checkpoint shards:  77%|███████▋  | 23/30 [00:45<00:19,  2.74s/it]Loading checkpoint shards:  77%|███████▋  | 23/30 [00:58<00:19,  2.75s/it]Loading checkpoint shards:  77%|███████▋  | 23/30 [00:57<00:19,  2.74s/it]Loading checkpoint shards:  77%|███████▋  | 23/30 [00:42<00:19,  2.74s/it]Loading checkpoint shards:  77%|███████▋  | 23/30 [00:55<00:19,  2.75s/it]Loading checkpoint shards:  77%|███████▋  | 23/30 [00:44<00:19,  2.74s/it]Loading checkpoint shards:  77%|███████▋  | 23/30 [00:42<00:19,  2.74s/it]Loading checkpoint shards:  77%|███████▋  | 23/30 [00:55<00:19,  2.75s/it]Loading checkpoint shards:  80%|████████  | 24/30 [00:48<00:16,  2.76s/it]Loading checkpoint shards:  80%|████████  | 24/30 [01:00<00:16,  2.76s/it]Loading checkpoint shards:  80%|████████  | 24/30 [01:00<00:16,  2.76s/it]Loading checkpoint shards:  80%|████████  | 24/30 [00:47<00:16,  2.76s/it]Loading checkpoint shards:  80%|████████  | 24/30 [00:45<00:16,  2.76s/it]Loading checkpoint shards:  80%|████████  | 24/30 [00:44<00:16,  2.76s/it]Loading checkpoint shards:  80%|████████  | 24/30 [00:57<00:16,  2.76s/it]Loading checkpoint shards:  80%|████████  | 24/30 [00:58<00:16,  2.76s/it]Loading checkpoint shards:  83%|████████▎ | 25/30 [00:50<00:13,  2.67s/it]Loading checkpoint shards:  83%|████████▎ | 25/30 [01:03<00:13,  2.67s/it]Loading checkpoint shards:  83%|████████▎ | 25/30 [01:02<00:13,  2.67s/it]Loading checkpoint shards:  83%|████████▎ | 25/30 [00:47<00:13,  2.67s/it]Loading checkpoint shards:  83%|████████▎ | 25/30 [00:49<00:13,  2.67s/it]Loading checkpoint shards:  83%|████████▎ | 25/30 [01:00<00:13,  2.67s/it]Loading checkpoint shards:  83%|████████▎ | 25/30 [00:47<00:13,  2.67s/it]Loading checkpoint shards:  83%|████████▎ | 25/30 [01:01<00:13,  2.67s/it]Loading checkpoint shards:  87%|████████▋ | 26/30 [01:06<00:11,  2.77s/it]Loading checkpoint shards:  87%|████████▋ | 26/30 [00:53<00:11,  2.77s/it]Loading checkpoint shards:  87%|████████▋ | 26/30 [01:05<00:11,  2.77s/it]Loading checkpoint shards:  87%|████████▋ | 26/30 [00:52<00:11,  2.77s/it]Loading checkpoint shards:  87%|████████▋ | 26/30 [00:50<00:11,  2.77s/it]Loading checkpoint shards:  87%|████████▋ | 26/30 [01:03<00:11,  2.77s/it]Loading checkpoint shards:  87%|████████▋ | 26/30 [00:50<00:11,  2.77s/it]Loading checkpoint shards:  87%|████████▋ | 26/30 [01:04<00:11,  2.77s/it]Loading checkpoint shards:  90%|█████████ | 27/30 [00:57<00:09,  3.11s/it]Loading checkpoint shards:  90%|█████████ | 27/30 [01:10<00:09,  3.11s/it]Loading checkpoint shards:  90%|█████████ | 27/30 [01:09<00:09,  3.11s/it]Loading checkpoint shards:  90%|█████████ | 27/30 [00:54<00:09,  3.11s/it]Loading checkpoint shards:  90%|█████████ | 27/30 [01:07<00:09,  3.11s/it]Loading checkpoint shards:  90%|█████████ | 27/30 [00:56<00:09,  3.11s/it]Loading checkpoint shards:  90%|█████████ | 27/30 [00:54<00:09,  3.11s/it]Loading checkpoint shards:  90%|█████████ | 27/30 [01:08<00:09,  3.11s/it]Loading checkpoint shards:  93%|█████████▎| 28/30 [01:00<00:05,  2.98s/it]Loading checkpoint shards:  93%|█████████▎| 28/30 [01:12<00:05,  2.98s/it]Loading checkpoint shards:  93%|█████████▎| 28/30 [01:12<00:05,  2.98s/it]Loading checkpoint shards:  93%|█████████▎| 28/30 [00:59<00:05,  2.98s/it]Loading checkpoint shards:  93%|█████████▎| 28/30 [00:56<00:05,  2.98s/it]Loading checkpoint shards:  93%|█████████▎| 28/30 [01:09<00:05,  2.98s/it]Loading checkpoint shards:  93%|█████████▎| 28/30 [00:57<00:05,  2.98s/it]Loading checkpoint shards:  93%|█████████▎| 28/30 [01:10<00:05,  2.98s/it]Loading checkpoint shards:  97%|█████████▋| 29/30 [01:14<00:02,  2.80s/it]Loading checkpoint shards:  97%|█████████▋| 29/30 [01:15<00:02,  2.80s/it]Loading checkpoint shards:  97%|█████████▋| 29/30 [01:02<00:02,  2.80s/it]Loading checkpoint shards:  97%|█████████▋| 29/30 [01:01<00:02,  2.80s/it]Loading checkpoint shards:  97%|█████████▋| 29/30 [00:59<00:02,  2.80s/it]Loading checkpoint shards:  97%|█████████▋| 29/30 [01:12<00:02,  2.80s/it]Loading checkpoint shards:  97%|█████████▋| 29/30 [00:59<00:02,  2.80s/it]Loading checkpoint shards:  97%|█████████▋| 29/30 [01:13<00:02,  2.80s/it]Loading checkpoint shards: 100%|██████████| 30/30 [01:02<00:00,  2.02s/it]Loading checkpoint shards: 100%|██████████| 30/30 [01:02<00:00,  2.10s/it]
Loading checkpoint shards: 100%|██████████| 30/30 [01:15<00:00,  2.02s/it]Loading checkpoint shards: 100%|██████████| 30/30 [01:15<00:00,  2.52s/it]
Loading checkpoint shards: 100%|██████████| 30/30 [01:14<00:00,  2.02s/it]Loading checkpoint shards: 100%|██████████| 30/30 [01:14<00:00,  2.49s/it]
Loading checkpoint shards: 100%|██████████| 30/30 [00:59<00:00,  2.02s/it]Loading checkpoint shards: 100%|██████████| 30/30 [00:59<00:00,  1.98s/it]
Loading checkpoint shards: 100%|██████████| 30/30 [01:02<00:00,  2.02s/it]Loading checkpoint shards: 100%|██████████| 30/30 [01:02<00:00,  2.07s/it]
Loading checkpoint shards: 100%|██████████| 30/30 [01:12<00:00,  2.02s/it]Loading checkpoint shards: 100%|██████████| 30/30 [01:12<00:00,  2.42s/it]
Loading checkpoint shards: 100%|██████████| 30/30 [00:59<00:00,  2.02s/it]Loading checkpoint shards: 100%|██████████| 30/30 [00:59<00:00,  1.99s/it]
All model checkpoint weights were used when initializing LlamaForCausalLM.

All the weights of LlamaForCausalLM were initialized from the model checkpoint at /orfeo/cephfs/scratch/area/ddoimo/models/llama_v3/models_hf/llama-3-70b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
Loading checkpoint shards: 100%|██████████| 30/30 [01:13<00:00,  2.02s/it]Loading checkpoint shards: 100%|██████████| 30/30 [01:13<00:00,  2.44s/it]
loading configuration file /orfeo/cephfs/scratch/area/ddoimo/models/llama_v3/models_hf/llama-3-70b/generation_config.json
Generate config GenerationConfig {
  "bos_token_id": 128000,
  "eos_token_id": 128001
}

model loading finished. 


Initializing LORA model...
trainable params: 828,375,040 || all params: 71,382,081,536 || trainable%: 1.160480364504679
loading file tokenizer.json
loading file added_tokens.json
loading file special_tokens_map.json
loading file tokenizer_config.json
max_seq_len:  2048
loading dataset
split: train
mode: dev+validation

Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
tokenization started
tokenization finished
num_samples = 1065
loading dataset
split: validation
mode: validation

tokenization started
tokenization finished
loading dataset
split: test
mode: test

tokenization started
tokenization finished
memory consumed before loading model
CUDA mem allocated: 9.5367431640625e-07 GB
CUDA mem reserved: 0.001953125 GB
memory consumed after loading model
CUDA mem allocated: 24.760630130767822 GB
CUDA mem reserved: 25.8828125 GB
preparing activation checkpointing..
setup scheduler and optimizer..
measuring baselines..
measuring validation accuracy
993/ 1531 inputs processed
inference throughput:  104.07 token/sec/gpu
06/17/2024 23:03:57 - INFO - __main__ - iter 0. mmlu val accuracy: macro 0.7682, micro 0.7421
start training
memory before train run
CUDA mem allocated: 24.760630130767822 GB
CUDA mem reserved: 28.380859375 GB
06/17/2024 23:03:57 - INFO - __main__ - ***** Running training *****
06/17/2024 23:03:57 - INFO - __main__ -   Num examples = 1065
06/17/2024 23:03:57 - INFO - __main__ -   Num Epochs = 4
06/17/2024 23:03:57 - INFO - __main__ -   Learning rate = 0.0002
06/17/2024 23:03:57 - INFO - __main__ -   Weight Decay = 0.0
06/17/2024 23:03:57 - INFO - __main__ -   Instantaneous batch size per device = 1
06/17/2024 23:03:57 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 16
06/17/2024 23:03:57 - INFO - __main__ -   Gradient Accumulation steps = 2
06/17/2024 23:03:57 - INFO - __main__ -   len_dataloader = 133
06/17/2024 23:03:57 - INFO - __main__ -   Total optimization steps = 268
06/17/2024 23:03:57 - INFO - __main__ -   Warmup steps = 13
06/17/2024 23:03:57 - INFO - __main__ -   Log steps number = 21
log step: 13/260
LR: 0.0002, Loss: 0.29912405747633714,                             Time: 0 h  1.62 min
log step: 26/260
LR: 0.00019873298211580336, Loss: 0.3524029805110051,                             Time: 0 h  3.14 min
measuring validation accuracy
993/ 1531 inputs processed
inference throughput:  105.25 token/sec/gpu
06/17/2024 23:10:49 - INFO - __main__ - iter 27. mmlu val accuracy: macro 0.7962, micro 0.7579
CUDA mem allocated: 26.885592937469482 GB
CUDA mem reserved: 29.115234375 GB
log step: 39/260
LR: 0.00019496435945955424, Loss: 0.32956281075110805,                             Time: 0 h  4.67 min
log step: 52/260
LR: 0.00018879059490608613, Loss: 0.32538135235126203,                             Time: 0 h  6.16 min
measuring validation accuracy
993/ 1531 inputs processed
inference throughput:  104.95 token/sec/gpu
06/17/2024 23:17:35 - INFO - __main__ - iter 54. mmlu val accuracy: macro 0.8110, micro 0.7585
CUDA mem allocated: 27.354118824005127 GB
CUDA mem reserved: 30.072265625 GB
log step: 65/260
LR: 0.0001803697141140266, Loss: 0.28977133677555966,                             Time: 0 h  7.66 min
processed  26.70 token/sec/gpu
measuring test accuracy
993/ 14042 inputs processed
1993/ 14042 inputs processed
2993/ 14042 inputs processed
3993/ 14042 inputs processed
4993/ 14042 inputs processed
5993/ 14042 inputs processed
6993/ 14042 inputs processed
7993/ 14042 inputs processed
8993/ 14042 inputs processed
9993/ 14042 inputs processed
10993/ 14042 inputs processed
11993/ 14042 inputs processed
12993/ 14042 inputs processed
13993/ 14042 inputs processed
inference throughput:  104.57 token/sec/gpu
06/17/2024 23:52:18 - INFO - __main__ - mmlu test accuracy after epoch 0: macro 0.7523, micro 0.7343
CUDA mem allocated: 27.354118824005127 GB
CUDA mem reserved: 30.072265625 GB
/orfeo/cephfs/scratch/area/ddoimo/finetuning_llm/open-instruct/env_amd_new/lib/python3.11/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /orfeo/cephfs/scratch/area/ddoimo/models/llama_v3/models_hf/llama-3-70b - will assume that the vocabulary was not modified.
  warnings.warn(
log step: 78/260
LR: 0.00016991726065032975, Loss: 0.2470362920027513,                             Time: 0 h  9.27 min
measuring validation accuracy
993/ 1531 inputs processed
inference throughput:  105.08 token/sec/gpu
06/17/2024 23:59:24 - INFO - __main__ - iter 81. mmlu val accuracy: macro 0.8708, micro 0.8141
CUDA mem allocated: 27.354118824005127 GB
CUDA mem reserved: 31.419921875 GB
log step: 91/260
LR: 0.00015770077886797002, Loss: 0.21318978529710036,                             Time: 0 h  10.79 min
log step: 104/260
LR: 0.00014403296575483056, Loss: 0.1220553838289701,                             Time: 0 h  12.29 min
measuring validation accuracy
993/ 1531 inputs processed
inference throughput:  105.00 token/sec/gpu
06/18/2024 00:06:09 - INFO - __main__ - iter 108. mmlu val accuracy: macro 0.8898, micro 0.8331
CUDA mem allocated: 27.354118824005127 GB
CUDA mem reserved: 31.419921875 GB
log step: 117/260
LR: 0.00012926366704125888, Loss: 0.14812753750727728,                             Time: 0 h  13.80 min
log step: 130/260
LR: 0.00011377092243649015, Loss: 0.06351225192730243,                             Time: 0 h  15.32 min
processed  13.35 token/sec/gpu
measuring test accuracy
993/ 14042 inputs processed
1993/ 14042 inputs processed
2993/ 14042 inputs processed
3993/ 14042 inputs processed
4993/ 14042 inputs processed
5993/ 14042 inputs processed
6993/ 14042 inputs processed
7993/ 14042 inputs processed
8993/ 14042 inputs processed
9993/ 14042 inputs processed
10993/ 14042 inputs processed
11993/ 14042 inputs processed
12993/ 14042 inputs processed
13993/ 14042 inputs processed
inference throughput:  104.63 token/sec/gpu
06/18/2024 00:42:21 - INFO - __main__ - mmlu test accuracy after epoch 1: macro 0.7686, micro 0.7570
CUDA mem allocated: 27.354118824005127 GB
CUDA mem reserved: 31.419921875 GB
measuring validation accuracy
993/ 1531 inputs processed
inference throughput:  104.45 token/sec/gpu
06/18/2024 00:48:05 - INFO - __main__ - iter 135. mmlu val accuracy: macro 0.8923, micro 0.8429
CUDA mem allocated: 27.354118824005127 GB
CUDA mem reserved: 31.419921875 GB
log step: 143/260
LR: 9.795128920293916e-05, Loss: 0.17842490856464094,                             Time: 0 h  16.94 min
log step: 156/260
LR: 8.220969174925925e-05, Loss: 0.1373898983001709,                             Time: 0 h  18.45 min
measuring validation accuracy
993/ 1531 inputs processed
inference throughput:  105.10 token/sec/gpu
06/18/2024 00:54:50 - INFO - __main__ - iter 162. mmlu val accuracy: macro 0.9232, micro 0.8658
CUDA mem allocated: 27.354118824005127 GB
CUDA mem reserved: 31.419921875 GB
log step: 169/260
LR: 6.694905705524112e-05, Loss: 0.07182531631909884,                             Time: 0 h  19.97 min
log step: 182/260
LR: 5.256000122354158e-05, Loss: 0.05943740331209623,                             Time: 0 h  21.49 min
measuring validation accuracy
993/ 1531 inputs processed
inference throughput:  104.79 token/sec/gpu
06/18/2024 01:01:39 - INFO - __main__ - iter 189. mmlu val accuracy: macro 0.9237, micro 0.8671
CUDA mem allocated: 27.354118824005127 GB
CUDA mem reserved: 31.419921875 GB
log step: 195/260
LR: 3.94108311445741e-05, Loss: 0.05713691619726328,                             Time: 0 h  23.02 min
processed  8.89 token/sec/gpu
measuring test accuracy
993/ 14042 inputs processed
1993/ 14042 inputs processed
2993/ 14042 inputs processed
3993/ 14042 inputs processed
4993/ 14042 inputs processed
5993/ 14042 inputs processed
6993/ 14042 inputs processed
7993/ 14042 inputs processed
8993/ 14042 inputs processed
9993/ 14042 inputs processed
10993/ 14042 inputs processed
11993/ 14042 inputs processed
12993/ 14042 inputs processed
13993/ 14042 inputs processed
inference throughput:  104.64 token/sec/gpu
06/18/2024 01:36:06 - INFO - __main__ - mmlu test accuracy after epoch 2: macro 0.7807, micro 0.7791
CUDA mem allocated: 27.354118824005127 GB
CUDA mem reserved: 31.419921875 GB
log step: 208/260
LR: 2.783811719515476e-05, Loss: 0.10632564471318172,                             Time: 0 h  24.77 min
measuring validation accuracy
993/ 1531 inputs processed
inference throughput:  103.42 token/sec/gpu
06/18/2024 01:43:51 - INFO - __main__ - iter 216. mmlu val accuracy: macro 0.9323, micro 0.8763
CUDA mem allocated: 27.354118824005127 GB
CUDA mem reserved: 31.419921875 GB
log step: 221/260
LR: 1.8138078275134206e-05, Loss: 0.04850992331138024,                             Time: 0 h  26.37 min
log step: 234/260
LR: 1.0558999693382504e-05, Loss: 0.015420885040209843,                             Time: 0 h  27.93 min
measuring validation accuracy
993/ 1531 inputs processed
inference throughput:  104.41 token/sec/gpu
06/18/2024 01:50:46 - INFO - __main__ - iter 243. mmlu val accuracy: macro 0.9325, micro 0.8763
CUDA mem allocated: 27.354118824005127 GB
CUDA mem reserved: 31.419921875 GB
log step: 247/260
LR: 5.294877977350445e-06, Loss: 0.0024054887203069832,                             Time: 0 h  29.51 min
log step: 260/260
LR: 2.4804552757006215e-06, Loss: 0.07503509521484375,                             Time: 0 h  31.12 min
processed  6.57 token/sec/gpu
measuring test accuracy
993/ 14042 inputs processed
1993/ 14042 inputs processed
2993/ 14042 inputs processed
3993/ 14042 inputs processed
4993/ 14042 inputs processed
5993/ 14042 inputs processed
6993/ 14042 inputs processed
7993/ 14042 inputs processed
8993/ 14042 inputs processed
9993/ 14042 inputs processed
10993/ 14042 inputs processed
11993/ 14042 inputs processed
12993/ 14042 inputs processed
13993/ 14042 inputs processed
inference throughput:  103.37 token/sec/gpu
06/18/2024 02:27:11 - INFO - __main__ - mmlu test accuracy after epoch 3: macro 0.7776, micro 0.7746
CUDA mem allocated: 27.354118824005127 GB
CUDA mem reserved: 31.419921875 GB
